name: scraper

on:
  workflow_dispatch:
    inputs:
      max_delta_t:
        description: "max_delta_t"
        required: false
        default: "10"
  schedule:
    - cron: "30 * * * *"

jobs:
  scraper:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo branch main
        uses: actions/checkout@v4
        with:
          ref: "main"
          token: "${{ secrets.REPO_DATA_TOKEN }}"

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install ghostscript
          python -m pip install --upgrade pip
          python -m pip install flake8 pytest
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install build

      - name: "python workflows/scraper.py"
        run: |
          export PYTHONPATH="$PYTHONPATH:./src"

          git config --global user.email "${{ secrets.GIT_USER_EMAIL }}"
          git config --global user.name "${{ secrets.GIT_USER_NAME }}"
          python workflows/scraper.py ${{ github.event.inputs.max_delta_t }}

      - name: Push
        run: |
          git add .
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi
          git commit -m "ðŸ¤– [scraper.yml] $(date '+%Y-%m-%d-%H%M')"
          git pull origin main --rebase
          git push origin main
